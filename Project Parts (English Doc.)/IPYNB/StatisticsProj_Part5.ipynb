{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import e\n",
    "from scipy.stats import beta, norm, bernoulli\n",
    "from numpy import linalg\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "execution_count":136,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"5Zf23NPTveLcR21EAN0AUV",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "data = pd.read_csv(\"athletes_edited.csv\")\n",
    "display(data)"
   ],
   "execution_count":137,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>nationality<\/th>\n",
       "      <th>sport<\/th>\n",
       "      <th>age<\/th>\n",
       "      <th>male<\/th>\n",
       "      <th>height<\/th>\n",
       "      <th>weight<\/th>\n",
       "      <th>won_any<\/th>\n",
       "      <th>won_gold<\/th>\n",
       "      <th>total_medals<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>ESP<\/td>\n",
       "      <td>athletics<\/td>\n",
       "      <td>46<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>172<\/td>\n",
       "      <td>64<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1<\/th>\n",
       "      <td>KOR<\/td>\n",
       "      <td>fencing<\/td>\n",
       "      <td>29<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>168<\/td>\n",
       "      <td>56<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2<\/th>\n",
       "      <td>CAN<\/td>\n",
       "      <td>athletics<\/td>\n",
       "      <td>24<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>198<\/td>\n",
       "      <td>79<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3<\/th>\n",
       "      <td>MDA<\/td>\n",
       "      <td>taekwondo<\/td>\n",
       "      <td>25<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>183<\/td>\n",
       "      <td>80<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4<\/th>\n",
       "      <td>NZL<\/td>\n",
       "      <td>cycling<\/td>\n",
       "      <td>25<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>181<\/td>\n",
       "      <td>71<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>10853<\/th>\n",
       "      <td>CUB<\/td>\n",
       "      <td>athletics<\/td>\n",
       "      <td>20<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>164<\/td>\n",
       "      <td>58<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>10854<\/th>\n",
       "      <td>CZE<\/td>\n",
       "      <td>athletics<\/td>\n",
       "      <td>29<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>173<\/td>\n",
       "      <td>63<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>10855<\/th>\n",
       "      <td>CHN<\/td>\n",
       "      <td>wrestling<\/td>\n",
       "      <td>25<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>185<\/td>\n",
       "      <td>100<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>10856<\/th>\n",
       "      <td>VIE<\/td>\n",
       "      <td>weightlifting<\/td>\n",
       "      <td>27<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>160<\/td>\n",
       "      <td>56<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>10857<\/th>\n",
       "      <td>RSA<\/td>\n",
       "      <td>athletics<\/td>\n",
       "      <td>24<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>185<\/td>\n",
       "      <td>70<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>10858 rows Ã— 9 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"w0jduAXri4Cmz49zKDkvIo",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Part 1\n",
    "\n",
    "### From project 2\n",
    "\n",
    "**The research question -- Does the average weight (X) of the athletes change between the athletes who won a medal (Y=1) and the ones who didn't (Y=0)?**"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"qXv5lVNKZMWA11UjS4X1Wd",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Q1"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"n6N0SqVlbJ2MAzqg0qfMAP",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# we set random_state in order to get consistent results\n",
    "sampled_data = data.sample(n=200, random_state=0)\n",
    "sampled_indecies = sampled_data.index.values\n",
    "\n",
    "data_without_sampled = data.drop(sampled_indecies, axis=0)\n",
    "past_data = data_without_sampled.sample(n=1000, random_state=0)\n",
    "\n",
    "display(sampled_data)\n",
    "display(past_data)"
   ],
   "execution_count":138,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>nationality<\/th>\n",
       "      <th>sport<\/th>\n",
       "      <th>age<\/th>\n",
       "      <th>male<\/th>\n",
       "      <th>height<\/th>\n",
       "      <th>weight<\/th>\n",
       "      <th>won_any<\/th>\n",
       "      <th>won_gold<\/th>\n",
       "      <th>total_medals<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6798<\/th>\n",
       "      <td>GER<\/td>\n",
       "      <td>athletics<\/td>\n",
       "      <td>29<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>189<\/td>\n",
       "      <td>78<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2124<\/th>\n",
       "      <td>CUB<\/td>\n",
       "      <td>athletics<\/td>\n",
       "      <td>28<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>171<\/td>\n",
       "      <td>60<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>159<\/th>\n",
       "      <td>NIG<\/td>\n",
       "      <td>judo<\/td>\n",
       "      <td>28<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>180<\/td>\n",
       "      <td>73<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4148<\/th>\n",
       "      <td>EGY<\/td>\n",
       "      <td>judo<\/td>\n",
       "      <td>34<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>195<\/td>\n",
       "      <td>105<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1478<\/th>\n",
       "      <td>RSA<\/td>\n",
       "      <td>aquatics<\/td>\n",
       "      <td>25<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>158<\/td>\n",
       "      <td>86<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3000<\/th>\n",
       "      <td>HUN<\/td>\n",
       "      <td>aquatics<\/td>\n",
       "      <td>30<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>173<\/td>\n",
       "      <td>64<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1835<\/th>\n",
       "      <td>AUS<\/td>\n",
       "      <td>aquatics<\/td>\n",
       "      <td>17<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>162<\/td>\n",
       "      <td>62<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2818<\/th>\n",
       "      <td>RUS<\/td>\n",
       "      <td>aquatics<\/td>\n",
       "      <td>22<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>166<\/td>\n",
       "      <td>62<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>9643<\/th>\n",
       "      <td>CIV<\/td>\n",
       "      <td>aquatics<\/td>\n",
       "      <td>21<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>176<\/td>\n",
       "      <td>63<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1656<\/th>\n",
       "      <td>COL<\/td>\n",
       "      <td>cycling<\/td>\n",
       "      <td>22<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>178<\/td>\n",
       "      <td>65<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>200 rows Ã— 9 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    },
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>nationality<\/th>\n",
       "      <th>sport<\/th>\n",
       "      <th>age<\/th>\n",
       "      <th>male<\/th>\n",
       "      <th>height<\/th>\n",
       "      <th>weight<\/th>\n",
       "      <th>won_any<\/th>\n",
       "      <th>won_gold<\/th>\n",
       "      <th>total_medals<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8283<\/th>\n",
       "      <td>GBR<\/td>\n",
       "      <td>canoe<\/td>\n",
       "      <td>27<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>177<\/td>\n",
       "      <td>70<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3394<\/th>\n",
       "      <td>NZL<\/td>\n",
       "      <td>sailing<\/td>\n",
       "      <td>22<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>165<\/td>\n",
       "      <td>59<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>8475<\/th>\n",
       "      <td>CHI<\/td>\n",
       "      <td>archery<\/td>\n",
       "      <td>16<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>183<\/td>\n",
       "      <td>88<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>7793<\/th>\n",
       "      <td>UKR<\/td>\n",
       "      <td>aquatics<\/td>\n",
       "      <td>21<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>178<\/td>\n",
       "      <td>61<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>7474<\/th>\n",
       "      <td>USA<\/td>\n",
       "      <td>aquatics<\/td>\n",
       "      <td>27<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>199<\/td>\n",
       "      <td>102<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>4<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>8209<\/th>\n",
       "      <td>KAZ<\/td>\n",
       "      <td>gymnastics<\/td>\n",
       "      <td>18<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>170<\/td>\n",
       "      <td>63<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>7471<\/th>\n",
       "      <td>SWE<\/td>\n",
       "      <td>handball<\/td>\n",
       "      <td>25<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>167<\/td>\n",
       "      <td>62<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>10260<\/th>\n",
       "      <td>RUS<\/td>\n",
       "      <td>shooting<\/td>\n",
       "      <td>46<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>178<\/td>\n",
       "      <td>83<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4140<\/th>\n",
       "      <td>JPN<\/td>\n",
       "      <td>athletics<\/td>\n",
       "      <td>28<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>165<\/td>\n",
       "      <td>54<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>6414<\/th>\n",
       "      <td>MEX<\/td>\n",
       "      <td>athletics<\/td>\n",
       "      <td>27<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>162<\/td>\n",
       "      <td>47<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>1000 rows Ã— 9 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"WzGIS4FNY70Fx9a8oYNdlI",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Q2\n",
    "\n",
    "We will choose $\\tau$ to be the weight median of sampled_data"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"VpV7wuwmPQu0JwVc6w2KMz",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "tau = sampled_data[\"weight\"].median()\n",
    "print(f\"tau value: {tau}\")"
   ],
   "execution_count":139,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "tau value: 68.5\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"emluIS4xtPuebUhkw4mHWD",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Section A\n",
    "\n",
    "We will estimate the log risk ratio using plug-in estimator.\n",
    "\n",
    "We will find a confidence interval for log risk ratio using the bootstrap percentile method."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"tLZOzrZoJS4yh5hJGt0WCa",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "won_weight = sampled_data[sampled_data[\"won_any\"] == 1][\"weight\"]\n",
    "not_won_weight = sampled_data[sampled_data[\"won_any\"] == 0][\"weight\"]\n",
    "\n",
    "# the number of iterations in each simulation\n",
    "B = 500\n",
    "\n",
    "# the confidende level of the confidence intervals is 0.95\n",
    "alpha = 0.05\n",
    "\n",
    "\n",
    "def eta(p):\n",
    "    return math.log(p \/ (1 - p))\n",
    "\n",
    "\n",
    "def find_psi_hat(won_weight, not_won_weight):\n",
    "    p1_hat = sum([1 if x > tau else 0 for x in won_weight]) \/ len(won_weight)\n",
    "    p2_hat = sum([1 if x > tau else 0 for x in not_won_weight]) \/ len(not_won_weight)\n",
    "    log_risk_ratio_hat = eta(p1_hat) - eta(p2_hat)\n",
    "\n",
    "    return log_risk_ratio_hat\n",
    "\n",
    "\n",
    "def psi_bootstrap(won_weight, not_won_weight, B):\n",
    "    n, m = len(won_weight), len(not_won_weight)\n",
    "    bootstrap_lst = []\n",
    "\n",
    "    for b in range(B):\n",
    "        # we set random_state in order to get consistent results\n",
    "        won_weight_b = won_weight.sample(n=n, random_state=b, replace=True)\n",
    "        not_won_weight_b = not_won_weight.sample(n=m, random_state=b, replace=True)\n",
    "\n",
    "        psi_hat_b = find_psi_hat(won_weight_b ,not_won_weight_b)\n",
    "        bootstrap_lst.append(psi_hat_b)\n",
    "\n",
    "    return sorted(bootstrap_lst)\n",
    "\n",
    "psi_bootstrap_lst = psi_bootstrap(won_weight, not_won_weight, B)"
   ],
   "execution_count":140,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"WaeOiwcm3UoI28xNHZEx9i",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# find the estimator of log risk ratio based on sampled_data\n",
    "psi_hat = find_psi_hat(won_weight, not_won_weight)\n",
    "\n",
    "# find the confidence interval of log risk ratio with confidence level of 0.95 (as stated in the project)\n",
    "low_q = psi_bootstrap_lst[int((alpha \/ 2) * B)]\n",
    "high_q = psi_bootstrap_lst[int((1 - (alpha \/ 2)) * B)]\n",
    "confidence_interval = np.array([low_q, high_q])\n",
    "\n",
    "print(\"--- frequentistic ---\")\n",
    "print(f\"log risk ratio estimator: {round(psi_hat, 3)}\")\n",
    "print(f\"confidence interval for log risk ratio: {np.round(confidence_interval, 3)}\")"
   ],
   "execution_count":141,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "--- frequentistic ---\n",
      "log risk ratio estimator: 0.285\n",
      "confidence interval for log risk ratio: [-0.454  1.103]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"VnA2VlfZaKsdlPWHLKOyRk",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Section B\n",
    "\n",
    "Notations:\n",
    "\n",
    "- Let $Z_1$ be the same definition as $Z$, but refers only to the athletes' weight who won a medal. Note that $Z_1 \\sim Bernoulli(p_1)$\n",
    "\n",
    "- Let $n$ be the number of athletes who won a medal (from sampled data).\n",
    "\n",
    "- Let $S_1 = \\sum_{i=1}^{n}Z^{i}_{1}$\n",
    "\n",
    "- Let $Z_2$ be the same definition as $Z$, but refers only to the athletes' weight who didn't win a medal. Note that $Z_2 \\sim Bernoulli(p_2)$\n",
    "\n",
    "- Let $m$ be the number of athletes who didn't win any medal (from sampled data).\n",
    "\n",
    "- Let $S_2 = \\sum_{i=1}^{m}Z_{2}^{i}$\n",
    "\n",
    "First, let us define prior distributions for our parameters $p_1$ and $p_2$, as follows:\n",
    "\n",
    "$p_1, p_2 \\sim Uni[0, 1]$\n",
    "\n",
    "We saw in the lecture that if we have a uniform prior and the data parametric model is Bernoulli, then the posterior is $Beta(S+1, \\space n-S+1)$\n",
    "\n",
    "thus the posteriors of $p_1$ and $p_2$ are as follows:\n",
    "\n",
    "$p_1 | Z_1^n \\sim Beta(S_1+1, \\space n-S_1+1)$\n",
    "\n",
    "$p_2 | Z_2^m \\sim Beta(S_2+1, \\space m-S_2+1)$\n"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"howpppzDRAD3ZJ9cpCP8b6",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "n, m = len(won_weight), len(not_won_weight)\n",
    "s1 = sum([1 if x > tau else 0 for x in won_weight])\n",
    "s2 = sum([1 if x > tau else 0 for x in not_won_weight])\n",
    "\n",
    "\n",
    "def psi_simulate(a1, b1, a2, b2, B):\n",
    "    simulate_lst = []\n",
    "\n",
    "    for b in range(B):\n",
    "        # we set random_state in order to get consistent results\n",
    "        p1_b = beta.rvs(a1, b1, random_state=b)\n",
    "        p2_b = beta.rvs(a2, b2, random_state=b)\n",
    "\n",
    "        psi_b = eta(p1_b) - eta(p2_b)\n",
    "        simulate_lst.append(psi_b)\n",
    "\n",
    "    return sorted(simulate_lst)"
   ],
   "execution_count":142,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"vmlGwUiWDOzS8v0aWVQTYo",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "a1, b1 = s1+1, n-s1+1\n",
    "a2, b2 = s2+1, m-s2+1\n",
    "\n",
    "psi_simulate_lst = psi_simulate(a1, b1, a2, b2, B)\n",
    "\n",
    "# approximate the expectation using simulation as shown in the course's book\n",
    "psi_mean_approx = sum(psi_simulate_lst) \/ B\n",
    "\n",
    "# find credible interval as shown in the course's book\n",
    "low_q = psi_simulate_lst[int((alpha \/ 2) * B)]\n",
    "high_q = psi_simulate_lst[int((1 - (alpha \/ 2)) * B)]\n",
    "credible_interval = np.array([low_q, high_q])\n",
    "\n",
    "print(\"--- bayesian - unifrom prior ---\")\n",
    "print(f\"log risk ratio estimator: {round(psi_mean_approx, 3)}\")\n",
    "print(f\"credible interval for log risk ratio: {np.round(credible_interval, 3)}\")"
   ],
   "execution_count":143,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "--- bayesian - unifrom prior ---\n",
      "log risk ratio estimator: 0.273\n",
      "credible interval for log risk ratio: [-0.068  0.641]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"6vSVHBtHI10RP7Z1Wbl9G9",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Section C\n",
    "\n",
    "First, we will define the prior distributions for our parameters to be Jeffreys' prior.\n",
    "\n",
    "We saw in the lecture that when the data parametric model is Bernoulli then the prior is as follows: $\\sqrt{1 \/ p(1-p)}$\n",
    "\n",
    "Thus, the priors for $p_1$ and $p_2$ are $\\sqrt{1 \/ p_1(1-p_1)}$ and $\\sqrt{1 \/ p_2(1-p_2)}$ correspondingly.\n",
    "\n",
    "After calculation **(*)**, we found that the posterior distributions for $p_1$ and $p_2$ are as follows:\n",
    "\n",
    "$p_1 | Z_1^n \\sim Beta(S_1+\\frac{1}{2}, \\space n-S_1+\\frac{1}{2})$\n",
    "\n",
    "$p_2 | Z_2^m \\sim Beta(S_2+\\frac{1}{2}, \\space m-S_2+\\frac{1}{2})$\n",
    "\n",
    "$\\space$\n",
    "\n",
    "Calculation **(*)**:\n",
    "\n",
    "For $p_1 | Z_1^n$:\n",
    "\n",
    "### $f(p_1 | Z_1^n) \\propto L_n(p_1) \\pi(p_1) = \\sqrt{\\frac{1}{p_1(1-p_1)}} * p_1^{S_1}(1-p_1)^{n-S_1} = p_1^{S_1-\\frac{1}{2}} (1 - p_1)^{n-S_1-\\frac{1}{2}} = p_1^{(S_1+\\frac{1}{2})-1} (1 - p_1)^{(n-S_1+\\frac{1}{2})-1}$\n",
    "\n",
    "As we can see, the density function we found is that of a Beta distribution with the parameters mentioned above (up to a constant).\n",
    "\n",
    "The same calculation applies to $p_2 | Z_2^m$ as well. Hence, $p_2 | Z_2^m$ is distributed Beta with the parameters above."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"lZUIiEYUHxbVSelElZHLkJ",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "a1, b1 = s1 + 0.5, n - s1 + 0.5\n",
    "a2, b2 = s2 + 0.5, m - s2 + 0.5\n",
    "\n",
    "psi_simulate_lst = psi_simulate(a1, b1, a2, b2, B)\n",
    "\n",
    "# approximate the expectation using simulation as shown in the course's book\n",
    "psi_mean_approx = sum(psi_simulate_lst) \/ B\n",
    "\n",
    "# find credible interval as shown in the course's book\n",
    "low_q = psi_simulate_lst[int((alpha \/ 2) * B)]\n",
    "high_q = psi_simulate_lst[int((1 - (alpha \/ 2)) * B)]\n",
    "credible_interval = np.array([low_q, high_q])\n",
    "\n",
    "print(\"--- bayesian - Jeffreys' prior ---\")\n",
    "print(f\"log risk ratio estimator: {round(psi_mean_approx, 3)}\")\n",
    "print(f\"credible interval for log risk ratio: {np.round(credible_interval, 3)}\")"
   ],
   "execution_count":144,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "--- bayesian - Jeffreys' prior ---\n",
      "log risk ratio estimator: 0.28\n",
      "credible interval for log risk ratio: [-0.07   0.658]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"Po6HFGpy4FxdCYMvRaGFv4",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Section D\n",
    "\n",
    "As approved in the forum, we will do the following:\n",
    "\n",
    "First, we will assume $Uni(0, \\space 1)$ priors for $p_1$ and $p_2$.\n",
    "\n",
    "Then we will find the posterior based on the past data. as mentioned in section B, the posteriors will be:\n",
    "\n",
    "$p_1 | Z_1^n \\sim Beta(S_1^{past}+1, \\space n^{past}-S_1^{past}+1)$, where $S_1^{past}$ and $n^{past}$ are based on the past data.\n",
    "\n",
    "$p_2 | Z_2^m \\sim Beta(S_2^{past}+1, \\space m^{past}-S_2^{past}+1)$, where $S_2^{past}$ and $m^{past}$ are based on the past data.\n",
    "\n",
    "Now, we will refer to the posteriors based on the past data as our priors to $p_1$ and $p_2$ when dealing with 'sampled_data' (the priors we got are indeed in the Beta distribution family as mentioned in the question).\n",
    "\n",
    "As we saw in the lecture, $Beta(\\alpha, \\space \\beta)$ and $Bernoulli(p)$ classes are conjugate, with the posterior of the form $Beta(\\alpha + S, \\space \\beta + n - S)$\n",
    "\n",
    "Thus, the final posteriors will be as follows:\n",
    "\n",
    "$p_1 | Z_1^n \\sim Beta([S_1^{past}+1] + [S_1^{new}], \\space [n^{past}-S_1^{past}+1] + [n^{new}-S_1^{new}])$, where $S_1^{new}$ and $n^{new}$ are based on the 'sampled_data'.\n",
    "\n",
    "$p_2 | Z_2^m \\sim Beta([S_2^{past}+1] + [S_2^{new}], \\space [m^{past}-S_2^{past}+1] + [m^{new}-S_2^{new}])$, where $S_2^{new}$ and $m^{new}$ are based on the 'sampled_data'."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"fRbkilNFJkwof696DYotrt",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# find the posteriors of p_1 and p_2 beta parameters \n",
    "past_data_won_weight = past_data[past_data[\"won_any\"] == 1][\"weight\"]\n",
    "past_data_not_won_weight = past_data[past_data[\"won_any\"] == 0][\"weight\"]\n",
    "\n",
    "n_past, m_past = len(past_data_won_weight), len(past_data_not_won_weight)\n",
    "n_new, m_new = len(won_weight), len(not_won_weight)\n",
    "\n",
    "s1_past = sum([1 if x > tau else 0 for x in past_data_won_weight])\n",
    "s2_past = sum([1 if x > tau else 0 for x in past_data_not_won_weight])\n",
    "\n",
    "s1_new = sum([1 if x > tau else 0 for x in won_weight])\n",
    "s2_new = sum([1 if x > tau else 0 for x in not_won_weight])\n",
    "\n",
    "# the parameters of the posteriors of p1, p2\n",
    "a1 = (s1_past + 1) + (s1_new)\n",
    "b1 = (n_past - s1_past + 1) + (n_new - s1_new)\n",
    "\n",
    "a2 = (s2_past + 1) + (s2_new)\n",
    "b2 = (m_past - s2_past + 1) + (m_new - s2_new)"
   ],
   "execution_count":145,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"5Nm2VUcvvaWm4KBkUSuMjc",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "psi_simulate_lst = psi_simulate(a1, b1, a2, b2, B)\n",
    "\n",
    "# approximate the expectation using simulation as shown in the course's book\n",
    "psi_mean_approx = sum(psi_simulate_lst) \/ B\n",
    "\n",
    "# find credible interval as shown in the course's book\n",
    "low_q = psi_simulate_lst[int((alpha \/ 2) * B)]\n",
    "high_q = psi_simulate_lst[int((1 - (alpha \/ 2)) * B)]\n",
    "credible_interval = np.array([low_q, high_q])\n",
    "\n",
    "print(\"--- bayesian - Beta distribution family ---\")\n",
    "print(f\"log risk ratio estimator (bayesian) {round(psi_mean_approx, 3)}\")\n",
    "print(f\"credible interval for log risk ratio: {np.round(credible_interval, 3)}\")"
   ],
   "execution_count":146,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "--- bayesian - Beta distribution family ---\n",
      "log risk ratio estimator (bayesian) 0.228\n",
      "credible interval for log risk ratio: [0.082 0.386]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"HQ9cV5ENJQktLiAj8TLqGs",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Section E\n",
    "\n",
    "In section A, we estimated the log risk ratio using the frequentistic approach. On the other hand, in sections B-D we estimated the log risk ratio using the Bayesian approach.\n",
    "\n",
    "- In section A, we chose a plug-in estimator and the estimator's value is 0.285\n",
    "\n",
    "- In section B, we chose uniform priors $Uni(0, \\space 1)$ for $p_1$ and $p_2$, and the estimator's value is 0.273\n",
    "\n",
    "- In section C, we chose Jeffreys' prior for $p_1$ and $p_2$, and the estimator's value is 0.28\n",
    "\n",
    "- In section D, we chose priors from $Beta$ distribution family for $p_1$ and $p_2$, and the estimator's value is 0.228\n",
    "\n",
    "As we can see, the estimators we got in all sections are very similar, though the estimator in section A uses a different approach from the estimators in sections B to D. Moreover, the estimators in sections B and C assume different priors. In addition, the estimator in section D make use of past data (the procedure is described in section D).\n",
    "\n",
    "To conclude, all the methods produced similar results, but not identical. Therefore, choosing a statistical approach (frequentistic or Bayesian) and choosing a prior may influence the results."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"wy9oMiwJmFbxUj99umwOJQ",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Part 2\n",
    "\n",
    "The explaining variables we chose:\n",
    "- height (continuous) - represents the height (in cm) of the athlete. This variable has many values, and therefore we can consider it as a continuous variable by the project instructions.\n",
    "\n",
    "- age (continuous) - represents the age (in years) of the athlete. This variable has many values, and therefore we can consider it as a continuous variable by the project instructions.\n",
    "\n",
    "- male (discrete) - represents the gender of the athlete. This variable has only two values, 1 if the athlete is male, and 0 if the athlete is female. It is a binary variable and therefore discrete.\n",
    "\n",
    "The explained variable we chose:\n",
    "- weight (continuous) - represents the weight (in kg) of the athlete. This variable has many values, and therefore we can consider it as a continuous variable by the project instructions.\n",
    "\n",
    "(*) this part was taken from project 3."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"drPwEggmu9FupyRD9qEfvj",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# functions from project 3 we will use in this project\n",
    "\n",
    "def extract_X_y(data, X_columns, y_column):\n",
    "    # create X matrix with first column as bias column\n",
    "    X = data[X_columns]\n",
    "    X.insert(loc=0, column=\"bias\", value=[1 for i in range(len(X))])\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    # create y vector\n",
    "    y = data[y_column].to_numpy()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def find_beta_hat(X, y):\n",
    "    beta_hat = linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return beta_hat\n",
    "\n",
    "\n",
    "def find_sum_squares(beta_hat, X, y):\n",
    "  y_avg = np.mean(y)\n",
    "  SSR, SSE = 0, 0\n",
    "  for xi, yi in zip(X, y):\n",
    "    y_hat = beta_hat.T @ xi\n",
    "    SSR += (y_hat - y_avg) ** 2\n",
    "    SSE += (y_hat - yi) ** 2\n",
    "\n",
    "  SST = SSR + SSE\n",
    "\n",
    "  return SSR, SSE, SST\n",
    "\n",
    "\n",
    "def find_var_hat(beta_hat, X, y):\n",
    "    _, SSE, _ = find_sum_squares(beta_hat, X, y)\n",
    "    n, p = X.shape\n",
    "    MSE = SSE \/ (n - p)\n",
    "\n",
    "    return MSE * linalg.inv(X.T @ X)\n",
    "\n",
    "\n",
    "def find_confidence_intervals(beta_hat, var_hat):\n",
    "  z = norm.ppf(0.975)\n",
    "\n",
    "  for i, beta_i_hat in enumerate(beta_hat):\n",
    "    se_i_hat = var_hat[i][i] ** 0.5\n",
    "    confidence_interval = [round(beta_i_hat - (z * se_i_hat), 3), round(beta_i_hat + (z * se_i_hat), 3)]\n",
    "    print(f\"confidence interval for beta {i}:\")\n",
    "    print(confidence_interval)\n",
    "    print()"
   ],
   "execution_count":147,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"YqD8G57S3Rm3GDQDJdqO7z",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Q1\n",
    "\n",
    "As mentioend in projcet 1, we removed all records with missing values. Thus, we can just sample from our data."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"SAfLQ37WKsGLCQslbeHXrj",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# we set random_state in order to get consistent results\n",
    "sampled_data2 = data.sample(n=1000, random_state=0)"
   ],
   "execution_count":148,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"L15x1PHBHYvGQxOlKB658V",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Q2"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"mct7LbY3dG04dq38i7OLBr",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "X, y = extract_X_y(sampled_data2, X_columns=[\"height\", \"age\", \"male\"], y_column=\"weight\")\n",
    "beta_hat = find_beta_hat(X, y)\n",
    "\n",
    "print(\"---- Full data ----\")\n",
    "print(\"Mean square error estimator for beta (based on sampled_data2):\")\n",
    "print(list(np.around(beta_hat, 3)))"
   ],
   "execution_count":149,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "---- Full data ----\n",
      "Mean square error estimator for beta (based on sampled_data2):\n",
      "[-98.876, 0.939, 0.076, 6.197]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"g16Mpzp5w0tfR1SXh6LbtP",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "_, SSE, _ = find_sum_squares(beta_hat, X, y)\n",
    "n, p = X.shape\n",
    "MSE = SSE \/ (n-p)\n",
    "\n",
    "var_hat = MSE * linalg.inv(X.T @ X)\n",
    "\n",
    "print(\"confidence intervals using the entire 'sampled_data2'\\n\")\n",
    "find_confidence_intervals(beta_hat, var_hat)"
   ],
   "execution_count":150,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "confidence intervals using the entire 'sampled_data2'\n",
      "\n",
      "confidence interval for beta 0:\n",
      "[-110.418, -87.335]\n",
      "\n",
      "confidence interval for beta 1:\n",
      "[0.871, 1.006]\n",
      "\n",
      "confidence interval for beta 2:\n",
      "[-0.047, 0.199]\n",
      "\n",
      "confidence interval for beta 3:\n",
      "[4.695, 7.7]\n",
      "\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"2CZD1vJkZ9217wFyb1ehnm",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Q3\n",
    "\n",
    "Although our y variable has repeated values, there are very few repetitions, so we were permitted to refer to our values as unique (approved in the question forum)."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"JmFQk7VvfDVbGHewMDgCzs",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# find indicies to remove\n",
    "remove_indicies = []\n",
    "\n",
    "for i in range(1000):\n",
    "    # the higher the index of the record, the higher the y value\n",
    "    # therefore the higher the probabilty to remove the y value\n",
    "    p_i = i \/ 1000\n",
    "\n",
    "    # we set random_state in order to get consistent results\n",
    "    if bernoulli.rvs(p_i, random_state=i):\n",
    "        remove_indicies.append(i)\n",
    "\n",
    "# create the missing values data\n",
    "y_sorted = sorted(y)\n",
    "y_removed = []\n",
    "\n",
    "for i, val in enumerate(y_sorted):\n",
    "\n",
    "    if i in remove_indicies:\n",
    "        y_removed.append(None)\n",
    "        continue\n",
    "    \n",
    "    y_removed.append(val)\n",
    "\n",
    "sampled_data2_removed = sampled_data2.sort_values(\"weight\")\n",
    "sampled_data2_removed[\"weight\"] = y_removed\n",
    "\n",
    "display(sampled_data2_removed)\n",
    "X_removed, y_removed = extract_X_y(sampled_data2_removed, X_columns=[\"height\", \"age\", \"male\"], y_column=\"weight\")"
   ],
   "execution_count":151,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>nationality<\/th>\n",
       "      <th>sport<\/th>\n",
       "      <th>age<\/th>\n",
       "      <th>male<\/th>\n",
       "      <th>height<\/th>\n",
       "      <th>weight<\/th>\n",
       "      <th>won_any<\/th>\n",
       "      <th>won_gold<\/th>\n",
       "      <th>total_medals<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10460<\/th>\n",
       "      <td>CHN<\/td>\n",
       "      <td>gymnastics<\/td>\n",
       "      <td>16<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>140<\/td>\n",
       "      <td>33.0<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>10562<\/th>\n",
       "      <td>CHN<\/td>\n",
       "      <td>gymnastics<\/td>\n",
       "      <td>16<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>151<\/td>\n",
       "      <td>35.0<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>188<\/th>\n",
       "      <td>JPN<\/td>\n",
       "      <td>gymnastics<\/td>\n",
       "      <td>16<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>146<\/td>\n",
       "      <td>35.0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>7198<\/th>\n",
       "      <td>JPN<\/td>\n",
       "      <td>athletics<\/td>\n",
       "      <td>20<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>154<\/td>\n",
       "      <td>39.0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2228<\/th>\n",
       "      <td>BRA<\/td>\n",
       "      <td>gymnastics<\/td>\n",
       "      <td>31<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>147<\/td>\n",
       "      <td>40.0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>7014<\/th>\n",
       "      <td>USA<\/td>\n",
       "      <td>athletics<\/td>\n",
       "      <td>30<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>176<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1466<\/th>\n",
       "      <td>ESP<\/td>\n",
       "      <td>athletics<\/td>\n",
       "      <td>32<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>204<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1136<\/th>\n",
       "      <td>SRB<\/td>\n",
       "      <td>athletics<\/td>\n",
       "      <td>31<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>187<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>10521<\/th>\n",
       "      <td>TUR<\/td>\n",
       "      <td>shooting<\/td>\n",
       "      <td>29<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>185<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>6607<\/th>\n",
       "      <td>EST<\/td>\n",
       "      <td>weightlifting<\/td>\n",
       "      <td>25<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>185<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>1000 rows Ã— 9 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"YIbJnewjl60sC0zUxf47N2",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Q4\n",
    "\n",
    "### Section A"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"0vED2MO7RRciMqNhiAWcmZ",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# remove all records with missing values\n",
    "sampled_data2_complete = sampled_data2_removed.dropna()\n",
    "X_complete, y_complete = extract_X_y(sampled_data2_complete, X_columns=[\"height\", \"age\", \"male\"], y_column=\"weight\")\n",
    "\n",
    "beta_hat_complete = find_beta_hat(X_complete, y_complete)\n",
    "var_hat_complete = find_var_hat(beta_hat_complete, X_complete, y_complete)\n",
    "\n",
    "print(\"based on the complete data\\n\")\n",
    "\n",
    "print(\"Mean square error estimator for beta:\")\n",
    "print(list(np.around(beta_hat_complete, 3)))\n",
    "\n",
    "print(\"\\nconfidence intervals\\n\")\n",
    "find_confidence_intervals(beta_hat_complete, var_hat_complete)"
   ],
   "execution_count":152,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "based on the complete data\n",
      "\n",
      "Mean square error estimator for beta:\n",
      "[-62.268, 0.729, -0.039, 4.688]\n",
      "\n",
      "confidence intervals\n",
      "\n",
      "confidence interval for beta 0:\n",
      "[-74.286, -50.249]\n",
      "\n",
      "confidence interval for beta 1:\n",
      "[0.658, 0.799]\n",
      "\n",
      "confidence interval for beta 2:\n",
      "[-0.153, 0.074]\n",
      "\n",
      "confidence interval for beta 3:\n",
      "[3.294, 6.081]\n",
      "\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"YOj2bshJtU98YTtXYLwT7u",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Section B"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"kmtKOyLELBwHeeVjCLtW1p",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# find the regression imputations of the missing values\n",
    "y_with_reg_imputations = []\n",
    "\n",
    "for xi, yi in zip(X_removed, y_removed):\n",
    "\n",
    "    # if yi is missing, impute its values using the linear regression prediction\n",
    "    if np.isnan(yi):\n",
    "        yi_pred = beta_hat_complete.T @ xi\n",
    "        y_with_reg_imputations.append(yi_pred)\n",
    "        continue\n",
    "\n",
    "    y_with_reg_imputations.append(yi)\n",
    "\n",
    "sampled_data_reg_imputation = sampled_data2_removed.copy()\n",
    "sampled_data_reg_imputation[\"weight\"] = y_with_reg_imputations"
   ],
   "execution_count":153,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"kQHS9TjuBBDINwLYqt2DRQ",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "X_reg_imputation, y_reg_imputation = extract_X_y(sampled_data_reg_imputation, X_columns=[\"height\", \"age\", \"male\"], y_column=\"weight\")\n",
    "\n",
    "beta_hat_reg_imputation = find_beta_hat(X_reg_imputation, y_reg_imputation)\n",
    "var_hat_reg_imputation = find_var_hat(beta_hat_reg_imputation, X_reg_imputation, y_reg_imputation)\n",
    "\n",
    "print(\"based on regression imputation \\n\")\n",
    "\n",
    "print(\"Mean square error estimator for beta (based on regression imputation):\")\n",
    "print(list(np.around(beta_hat_reg_imputation, 3)))\n",
    "\n",
    "print(\"\\nconfidence intervals\\n\")\n",
    "find_confidence_intervals(beta_hat_reg_imputation, var_hat_reg_imputation)"
   ],
   "execution_count":154,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "based on regression imputation \n",
      "\n",
      "Mean square error estimator for beta (based on regression imputation):\n",
      "[-62.268, 0.729, -0.039, 4.688]\n",
      "\n",
      "confidence intervals\n",
      "\n",
      "confidence interval for beta 0:\n",
      "[-67.695, -56.84]\n",
      "\n",
      "confidence interval for beta 1:\n",
      "[0.697, 0.76]\n",
      "\n",
      "confidence interval for beta 2:\n",
      "[-0.097, 0.019]\n",
      "\n",
      "confidence interval for beta 3:\n",
      "[3.981, 5.394]\n",
      "\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"q9Bszh34XE8hpeEiNsaEiI",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Notation:\n",
    "\n",
    "- Let $\\hat \\beta_{Complete}$ be the mean square estimator based on the complete data (removing all missing data).\n",
    "- Let $\\hat \\beta_{Reg \\_ imputation}$ be the mean square estimator based on regression imputation.\n",
    "\n",
    "As we can see, $\\hat \\beta_{Reg \\_ imputation} = \\hat \\beta_{Complete}$\n",
    "\n",
    "$\\hat \\beta_{Complete}$ is trivially optimal for the complete data. In addition, we impute the missing values using the regression estimation of $\\hat \\beta_{Complete}$. Therefore, the error of this estimator on the imputed values is 0 by the definition of the imputation. Thus, it is the optimal estimator for the imputed data as well.\n",
    "\n",
    "\n",
    "Moreover, the confidence intervals based on the two methods are different. One of the reasons is that the number of records in each method is different (the number of records under the regression imputation method is bigger since we consider the missing values as well [after imputation]).\n",
    " "
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"WfVpgtUsAwA5n1cpzJ7ALp",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Section C + D\n",
    "\n",
    "In this section, we will use multiple imputation methods as seen in the lecture.\n",
    "\n",
    "We saw in the lecture that the MI estimator is distributed asymptotically Normal. Therefore, we can use the Normal approximation confidence interval.\n",
    "\n",
    "(*) We will assume that y values are distributed $N(\\beta^{*T}X, \\space \\sigma^2_\\epsilon)$ as stated in the question (where $\\sigma^2_\\epsilon$ is the variance of the noise $\\epsilon$).\n",
    "\n",
    "(**) We will define $M = 100$"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"F9iDlrC0IMLgKcZowZr6mq",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "M = 100\n",
    "\n",
    "def multiple_imputation(X, y, beta_hat, std, M):\n",
    "    beta_hat_lst, var_hat_lst = [], []\n",
    "\n",
    "    for m in range(M):\n",
    "        y_new_m = []\n",
    "\n",
    "        for xi, yi in zip(X, y):\n",
    "            \n",
    "            # if the y value is missing we impute by sampling\n",
    "            if np.isnan(yi):\n",
    "                mean_i = beta_hat.T @ xi\n",
    "                # we set random_state in order to get consistent results\n",
    "                yi_pred = norm.rvs(loc=mean_i, scale=std, random_state=m)\n",
    "                y_new_m.append(yi_pred)\n",
    "                continue\n",
    "            \n",
    "            y_new_m.append(yi)\n",
    "\n",
    "        beta_hat_m = find_beta_hat(X, y_new_m)\n",
    "        var_hat_m = find_var_hat(beta_hat_m, X, y_new_m)\n",
    "\n",
    "        beta_hat_lst.append(beta_hat_m)\n",
    "        var_hat_lst.append(var_hat_m)\n",
    "\n",
    "    return beta_hat_lst, var_hat_lst\n",
    "\n",
    "\n",
    "def get_var_hat_beta_hat_MI(beta_hat_lst, var_hat_lst, beta_hat_MI, M):\n",
    "    # calculates variance using Robin's formula (while replacing Fisher's information\n",
    "    # with the variance^(-1))\n",
    "\n",
    "    var_hat_beta_hat_MI = []\n",
    "    for i, beta_hat_MI_i in enumerate(beta_hat_MI):\n",
    "        item1, item2 = 0, 0\n",
    "\n",
    "        for beta_hat, var_hat in zip(beta_hat_lst, var_hat_lst):\n",
    "            item1 += var_hat[i][i]\n",
    "            item2 += (beta_hat[i] - beta_hat_MI[i]) ** 2\n",
    "\n",
    "        item1 *= (1 \/ M)\n",
    "        item2 *= (M+1) \/ (M * (M - 1))\n",
    "        var_i_hat = item1 + item2\n",
    "        var_hat_beta_hat_MI.append(var_i_hat)\n",
    "\n",
    "    return var_hat_beta_hat_MI\n",
    "\n",
    "\n",
    "_, SSE_complete, _ = find_sum_squares(beta_hat_complete, X_complete, y_complete)\n",
    "n_complete, p_complete = X_complete.shape\n",
    "MSE_complete = SSE_complete \/ (n_complete - p_complete)\n",
    "\n",
    "\n",
    "beta_hat_lst , var_hat_lst = multiple_imputation(X_removed, y_removed, beta_hat_complete, \n",
    "                                                 MSE_complete ** 0.5, M)"
   ],
   "execution_count":155,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"pu8X00U3DKrB9e3imbpBmm",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "beta_hat_MI = sum(beta_hat_lst) \/ len(beta_hat_lst)\n",
    "\n",
    "var_hat_beta_hat_MI = get_var_hat_beta_hat_MI(beta_hat_lst, var_hat_lst, beta_hat_MI, M)\n",
    "temp = np.diag(var_hat_beta_hat_MI)\n",
    "\n",
    "print(\"based on Multiple Imputation\\n\")\n",
    "\n",
    "print(\"Mean square error estimator:\")\n",
    "print(list(np.around(beta_hat_MI, 3)))\n",
    "\n",
    "print(\"\\nconfidence intervals:\\n\")\n",
    "find_confidence_intervals(beta_hat_MI, temp)"
   ],
   "execution_count":156,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "based on Multiple Imputation\n",
      "\n",
      "Mean square error estimator:\n",
      "[-62.762, 0.732, -0.039, 4.711]\n",
      "\n",
      "confidence intervals:\n",
      "\n",
      "confidence interval for beta 0:\n",
      "[-103.979, -21.545]\n",
      "\n",
      "confidence interval for beta 1:\n",
      "[0.469, 0.995]\n",
      "\n",
      "confidence interval for beta 2:\n",
      "[-0.112, 0.034]\n",
      "\n",
      "confidence interval for beta 3:\n",
      "[2.604, 6.818]\n",
      "\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"U7zIXbwaT7lrK2mJdKCDmI",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Section E\n",
    "\n",
    "In this section, we will estimate $P(R=1|X_1, X_2, X_3)$ using logistic regression where $X_1, X_2, X_3$ are the explaning variables ('height', 'age', 'male').\n",
    "\n",
    "In order to achieve it, we will find the MLE estimator."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"plXnUQKwVOJyyGtXK8jNQZ",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def sigmoid(x):\n",
    "    return (e ** x) \/ (1 + e ** x)"
   ],
   "execution_count":157,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"vers5Hi9I7Xt4j3D5IuVKU",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# creating the R colmun: 0 if the data is missing, else 1\n",
    "def add_R_column(df):\n",
    "    df_LR = df.copy()\n",
    "\n",
    "    R_lst = []\n",
    "    for y in df_LR[\"weight\"]:\n",
    "        if np.isnan(y):\n",
    "            R_lst.append(0)\n",
    "        \n",
    "        else:\n",
    "            R_lst.append(1)\n",
    "\n",
    "    df_LR[\"R\"] = R_lst\n",
    "\n",
    "    return df_LR\n",
    "\n",
    "df_LR = add_R_column(sampled_data2_removed)\n",
    "X_LR, y_LR = extract_X_y(df_LR, X_columns=[\"height\", \"age\", \"male\"], y_column=\"R\")\n",
    "\n",
    "# find the MLE estimator\n",
    "reg = LogisticRegression(penalty=\"none\", fit_intercept=False).fit(X_LR, y_LR)\n",
    "beta_hat_LR = reg.coef_[0]\n",
    "\n",
    "print(\"Logistic regression: \\n\")\n",
    "print(f\"The model MLE estimator:\")\n",
    "print(list(np.around(beta_hat_LR, 3)))\n",
    "\n",
    "# the probability predictions of seeing the records\n",
    "pi_lst = [sigmoid(beta_hat_LR.T @ xi) for xi in X_LR]"
   ],
   "execution_count":158,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Logistic regression: \n",
      "\n",
      "The model MLE estimator:\n",
      "[20.424, -0.113, -0.013, -0.593]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"kcLpOOfNccbucRXkVoptJG",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Section F\n",
    "\n",
    "Let us display the linear regression problem at hand as a Least Squares problem:\n",
    "\n",
    "$LS: \\space (y - X \\beta)^T W (y - X \\beta)$\n",
    "\n",
    "Where $W$ is a diagonal matrix of size $n$ x $n$, and $W_{ii} = R_i \/ \\pi _i$ for $1 \\le i \\le n$ (we already found $R_i$, $\\pi _i$ for all $X_i$ , $1 \\le i \\le n$ in previous questions). Expanding the brackets, we get:\n",
    "\n",
    "$LS: \\space y^T W y - 2(X^T W y)^T \\beta + (X \\beta)^T W (X \\beta)$\n",
    "\n",
    "To find the LS estimator, we would like to minimize the LS optimization problem. So, we will take the derivative and set it equal to zero:\n",
    "\n",
    "$-2 X^T W y + 2(X^T W X) \\beta = 0$\n",
    "\n",
    "$(X^T W X) \\beta = X^T W y$\n",
    "\n",
    "$\\Rightarrow \\hat \\beta_{IPW} = (X^T W X)^{-1} X^T W y$"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"5lpj82jNhohFCpodnHLncT",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def find_beta_hat_IPW(X, y, W):\n",
    "    beta_hat_IPW = linalg.inv(X.T @ W @ X) @ X.T @ W @ np.nan_to_num(y)\n",
    "    return beta_hat_IPW\n",
    "\n",
    "weight_lst = [(Ri \/ pi) for pi, Ri in zip(pi_lst, df_LR[\"R\"])]\n",
    "beta_hat_IPW = find_beta_hat_IPW(X_removed, y_removed, W=np.diag(weight_lst))\n",
    "\n",
    "print(\"based on IPW \\n\")\n",
    "\n",
    "print(\"Mean square error estimator:\")\n",
    "print(list(np.around(beta_hat_IPW, 3)))"
   ],
   "execution_count":159,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "based on IPW \n",
      "\n",
      "Mean square error estimator:\n",
      "[-66.061, 0.747, -0.008, 4.357]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"PIi5yGFj7JCTHyYzfGBzHI",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Section G\n",
    "\n",
    "In this section, we will find confidence intervals using beta hat IPW estimator and bootstrap percentile method."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"IYAsIonM1tsjoWvPurMoX6",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def beta_hat_IPW_bootstrap(data_removed, B):\n",
    "    n = len(data_removed)\n",
    "    beta_hat_IPW_lst = []\n",
    "\n",
    "    for b in range(B):\n",
    "        # sample the bootstrap sample\n",
    "        # we set random_state in order to get consistent results\n",
    "        b_data_removed = data_removed.sample(n=n, replace=True, random_state=b)\n",
    "        b_df_LR = add_R_column(b_data_removed)\n",
    "\n",
    "        # perform logistic regression on the bootstrapped sample\n",
    "        b_X_LR, b_y_LR = extract_X_y(b_df_LR, X_columns=[\"height\", \"age\", \"male\"], y_column=\"R\")\n",
    "        b_reg = LogisticRegression(penalty=\"none\", fit_intercept=False).fit(b_X_LR, b_y_LR)\n",
    "        b_beta_hat_LR = b_reg.coef_[0]\n",
    "        \n",
    "        # find weight list\n",
    "        b_weight_lst = [(Ri \/ sigmoid(b_beta_hat_LR.T @ xi)) for xi, Ri in zip(b_X_LR, b_df_LR[\"R\"])]\n",
    "\n",
    "        # extract linear regression columns\n",
    "        b_X, b_y = extract_X_y(b_df_LR, X_columns=[\"height\", \"age\", \"male\"], y_column=\"weight\")\n",
    "\n",
    "        # find beta hat IPW estimator (using the weight list found with logisitc regression)\n",
    "        b_beta_hat_IPW = find_beta_hat_IPW(b_X, b_y, W=np.diag(b_weight_lst))\n",
    "        beta_hat_IPW_lst.append(b_beta_hat_IPW)\n",
    "\n",
    "    return beta_hat_IPW_lst\n",
    "\n",
    "beta_hat_IPW_lst = beta_hat_IPW_bootstrap(sampled_data2_removed, B)"
   ],
   "execution_count":160,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"jc4uZ3I9gnn6jmx3II9QOZ",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "print(\"based on IPW\\n\")\n",
    "    \n",
    "for i in range(len(beta_hat_IPW)):\n",
    "    beta_hat_i_IPW_lst = sorted([beta[i] for beta in beta_hat_IPW_lst])\n",
    "    low_q = beta_hat_i_IPW_lst[int((alpha \/ 2) * B)]\n",
    "    high_q = beta_hat_i_IPW_lst[int((1 - (alpha \/ 2)) * B)]\n",
    "    \n",
    "    confidence_interval = [round(low_q, 3), round(high_q, 3)]\n",
    "    print(f\"confidence interval for beta {i}:\")\n",
    "    print(confidence_interval)\n",
    "\n",
    "    if i != len(beta_hat_IPW) - 1:\n",
    "        print()"
   ],
   "execution_count":161,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "based on IPW\n",
      "\n",
      "confidence interval for beta 0:\n",
      "[-78.553, -53.596]\n",
      "\n",
      "confidence interval for beta 1:\n",
      "[0.669, 0.825]\n",
      "\n",
      "confidence interval for beta 2:\n",
      "[-0.154, 0.142]\n",
      "\n",
      "confidence interval for beta 3:\n",
      "[2.737, 5.938]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"tbbrgwFXHBwj0I5CWiJOV1",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Section H\n",
    "\n",
    "In question 2, the estimator we got (before creating missing values), $\\hat \\beta = [-98.876, 0.939, 0.076, 6.197]$.\n",
    "\n",
    "After finding this estimator, we were asked in question 3 to create missing values in our data.\n",
    "\n",
    "In order to find an estimator to $\\beta^*$ while dealing with the missing data, we used the following methods:\n",
    "\n",
    "- Complete data (removing missing records) -- the estimator we got under this method $\\hat \\beta_{Complete} = [-62.268, 0.729, -0.039, 4.688]$.\n",
    "  \n",
    "- Regression imputation -- the estimator we got under this method $\\hat \\beta_{Reg\\_imputation} = [-62.268, 0.729, -0.039, 4.688]$.\n",
    "\n",
    "- Multiple imputation -- the estimator we got under this method $\\hat \\beta_{Multi\\_imputation} = [-62.762, 0.732, -0.039, 4.711]$.\n",
    "\n",
    "- IPW -- the estimator we got under this method $\\hat \\beta_{IPW} = [-66.061, 0.747, -0.008, 4.357]$.\n",
    "\n",
    "As we can see, none of the estimators above are very similar to the estimator on the full data. However, we would still like to see which of the methods has produced the estimator that's closest to $\\hat \\beta$.\n",
    "\n",
    "We will use Euclidean distance to compare the different estimators and select the one with the minimum Euclidean distance to $\\hat \\beta$).\n",
    "    "
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"GbHWNU42XJZY0IA6rkTp3x",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "method_estimator_lst = [(\"complete\", beta_hat_complete),\n",
    "                        (\"reg. imputation\", beta_hat_reg_imputation),\n",
    "                        (\"multi. imputation\", beta_hat_MI),\n",
    "                        (\"IPW\", beta_hat_IPW)]\n",
    "\n",
    "best_method = min(method_estimator_lst, key=lambda tup: linalg.norm(beta_hat - tup[1]))[0]\n",
    "print(f\"The method that produced the closet estimator to beta_hat: {best_method}\")"
   ],
   "execution_count":162,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "The method that produced the closet estimator to beta_hat : IPW\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"dmTweEpOTMVxFG27cOZNj4",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "The confidence intervals we received under this part:\n",
    "\n",
    "- Full data (before creating missing values):\n",
    "\n",
    "    [-110.418, -87.335], $\\space \\space$ [0.871, 1.006], $\\space \\space$ [-0.047, 0.199], $\\space \\space$ [4.695, 7.7]\n",
    "\n",
    "- Complete data (removing missing records):\n",
    "\n",
    "    [-74.286, -50.249],$\\space \\space \\space \\space \\space$ [0.658, 0.799], $\\space \\space$ [-0.153, 0.074], $\\space \\space$ [3.294, 6.081]\n",
    "\n",
    "- Regression Imputation:\n",
    "\n",
    "    [-67.695, -56.84], $\\space \\space \\space \\space \\space \\space$ [0.697, 0.76], $\\space \\space \\space \\space$ [-0.097, 0.019], $\\space \\space$ [3.981, 5.394]\n",
    "\n",
    "- Multiple imputation:\n",
    "\n",
    "    [-103.979, -21.545], $\\space \\space$ [0.469, 0.995], $\\space \\space$ [-0.112, 0.034], $\\space \\space$ [2.604, 6.818]\n",
    "\n",
    "- IPW:\n",
    "\n",
    "    [-78.553, -53.596], $\\space \\space \\space \\space$ [0.669, 0.825], $\\space \\space$ [-0.154, 0.142], $\\space \\space$ [2.737, 5.938]\n",
    "\n",
    "As we can see, the confidence intervals we received using the methods of estimation with missing data are rather different from the confidence interval we got using the full data and are also different from each other. Therefore, we can conclude that missing data has an impact on finding confidence intervals as well as the method of dealing with it.\n",
    "\n",
    "In our results:\n",
    "\n",
    "The confidence intervals for $\\beta_0, \\beta_1, \\beta_2, \\beta_3$ under regression imputation are the shortest among all methods.\n",
    "\n",
    "The confidence intervals for $\\beta_0, \\beta_1, \\beta_3$ under multiple imputation are the longest among all methods.\n",
    "\n",
    "The confidence interval for $\\beta_2$ under IPW is the longest among all methods."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"NPM3iqnMiY5A5EIdvyZT3i",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "version":1,
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    
   ]
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}